{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing Integrated Gradient Attribution. SCRATCH I kept crashing the kernel,so it might just not work on my cpu/mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewgordon/Documents/ML/TeamStefan/PeterLaiSAE/PLgpt-circuits/myenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "import pdb\n",
    "\n",
    "import os\n",
    "from models.gpt import GPT\n",
    "from models.sparsified import SparsifiedGPT, SparsifiedGPTOutput\n",
    "\n",
    "from config.gpt.training import options\n",
    "from config.sae.models import sae_options\n",
    "\n",
    "\n",
    "\n",
    "from data.tokenizers import ASCIITokenizer, TikTokenTokenizer\n",
    "\n",
    "from models.sae import SparseAutoencoder\n",
    "from typing import Callable\n",
    "from datasets import Dataset\n",
    "from data.dataloaders import TrainingDataLoader\n",
    "TensorFunction = Callable[[Tensor], Tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 shards for split val\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "t.mps.empty_cache()\n",
    "c_name = 'standardx8.shakespeare_64x4'\n",
    "name = 'standard.shakespeare_64x4'\n",
    "data_dir = 'data/shakespeare'\n",
    "batch_size = 1\n",
    "config = sae_options[c_name]\n",
    "\n",
    "\n",
    "model = SparsifiedGPT(config)\n",
    "model_path = os.path.join(\"checkpoints\", name)\n",
    "model = model.load(model_path, device=config.device)\n",
    "\n",
    "model.to(device)  # Move model\n",
    "\n",
    "\n",
    "ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\n",
    "if ddp:\n",
    "    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
    "    ddp_rank = int(os.environ[\"RANK\"])\n",
    "    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    device = t.device(f\"cuda:{ddp_local_rank}\")\n",
    "\n",
    "    assert torch.cuda.is_available()\n",
    "    t.cuda.set_device(device)\n",
    "else:\n",
    "    # vanilla, non-DDP run\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    device = config.device\n",
    "\n",
    "dataloader = TrainingDataLoader(\n",
    "    dir_path=data_dir,\n",
    "    B= batch_size,\n",
    "    T=model.config.block_size,\n",
    "    process_rank=ddp_rank,\n",
    "    num_processes=ddp_world_size,\n",
    "    split=\"val\",\n",
    ")\n",
    "x,y = dataloader.next_batch(device)\n",
    "\n",
    "input, _ = dataloader.next_batch(device) #get batch of inputs\n",
    "input.to(t.int32) #hacky to make thigs work on my M3, remove later\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = model.forward(input, targets=None, is_eval=True)\n",
    "feature_magnitudes0 = output.feature_magnitudes[0] #feature magnitudes at source layer (batchsize, seqlen, sae_encoding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_to_cpu(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_cpu(model):\n",
    "    device = t.device(\"cpu\")\n",
    "    for name, param in model.named_parameters():\n",
    "        param.to(device)\n",
    "    for saekey in model.saes:\n",
    "        model.saes[saekey].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer0 = 0\n",
    "layer1 = 1\n",
    "sae0 = model.saes[f'{layer0}']\n",
    "sae1 = model.saes[f'{layer1}']\n",
    "\n",
    "\n",
    "#define function that goes from feature magnitudes in layer0 to feature magnitudes in layer1\n",
    "class Sae0Decode(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return sae0.decode(x)\n",
    "    \n",
    "class Sae1Encode(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return sae1.encode(x)\n",
    "\n",
    "forward_list = [Sae0Decode()] + [model.gpt.transformer.h[i] for i in range(layer0, layer1)] + [Sae1Encode()]\n",
    "forward = t.nn.Sequential(*forward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, _ = dataloader.next_batch(device) #get batch of inputs\n",
    "\n",
    "input.to(t.int32) #hacky to make thigs work on my M3, remove later\n",
    "\n",
    "output = model.forward(input, targets=None, is_eval=True)\n",
    "feature_magnitudes0 = output.feature_magnitudes[layer0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0095, 0.0000,  ..., 0.0022, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0339, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0316, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(feature_magnitudes0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ig_attributions(model, 0, 1, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ig_attributions(model: SparsifiedGPT, layer0: int, layer1: int, ds: TrainingDataLoader, nbatches: int=32):\n",
    "    assert layer0 < layer1\n",
    "    assert layer0 >= 0\n",
    "    assert layer1 <= model.gpt.config.n_layer\n",
    "    sae0 = model.saes[f'{layer0}']\n",
    "    sae1 = model.saes[f'{layer1}']\n",
    "    #pdb.set_trace()\n",
    "\n",
    "    #define function that goes from feature magnitudes in layer0 to feature magnitudes in layer1\n",
    "    class Sae0Decode(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return sae0.decode(x)\n",
    "        \n",
    "    class Sae1Encode(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return sae1.encode(x)\n",
    "    \n",
    "    forward_list = [Sae0Decode()] + [model.gpt.transformer.h[i] for i in range(layer0, layer1)] + [Sae1Encode()]\n",
    "    forward = t.nn.Sequential(*forward_list)\n",
    "\n",
    "    \n",
    "    attributions = 0\n",
    "    for _ in range(nbatches):\n",
    "\n",
    "        input, _ = ds.next_batch(device) #get batch of inputs #TODO FIX THIS\n",
    "        \n",
    "        input.to(t.int32) #hacky to make thigs work on my M3, remove later\n",
    "        output = model.forward(input, targets=None, is_eval=True)\n",
    "        feature_magnitudes0 = output.feature_magnitudes[layer0] #feature magnitudes at source layer (batchsize, seqlen, sae_encoding_dim)\n",
    "        \n",
    "        \n",
    "        jacobian = integrate_gradient(feature_magnitudes0, None, forward) #(batch seq dim batch seq dim)\n",
    "        attributions = attributions + (jacobian **2).sum(dim=[0,1,3,4]) #Does this make sense? It feels incorrect to sum over position\n",
    "            #maybe a better output would have shape (seqlen, dim, seqlen, dim)?\n",
    "        \n",
    "\n",
    "\n",
    "    return attributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_gradient(x: Tensor, x_i: Tensor | None, y: TensorFunction, base:int= 0, steps:int=10):\n",
    "    \"\"\"\n",
    "    Approximates int_C d/dx_i y(z) dz where C is a linear path from base to x\n",
    "    :param x: End of path. It can be either a vector of feature magnitudes generated by the data, \n",
    "        or a one hot encoding of a feature magnitude. It should have a nonzero entry for x_i (???)\n",
    "        Shape (batchsize, seq_len, encoding)\n",
    "    :param x_i: Direction of partial derivative. It is often a one hot encoding of a given feature magnitude. \n",
    "        If none, this computes the entire Jacobian\n",
    "        Shape (batchzise, seq_len, encoding) ??Support just encoding??\n",
    "    :param y: Tensor valued function on the vector space containing x\n",
    "    :param base: Start of path. Default to 0\n",
    "    :param steps: Number of steps to use to approximate the integral\n",
    "    :return: Tensor of shape ((output of y), x.shape) if x_i = None else shape (output of y)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if base == 0:\n",
    "        base = t.zeros_like(x)\n",
    "    path = t.linspace(0, 1, steps)\n",
    "    steplength = t.linalg.norm(x - base, dim = -1, keepdim = True)/steps\n",
    "\n",
    "    integral = 0\n",
    "    for alpha in path:\n",
    "        point = (alpha*x + (1-alpha)*base).detach() #Find point on path\n",
    "        jac = jacobian(y, point)*steplength #compute jacobian of y wrt x, scale by length of a step\n",
    "        \n",
    "        if x_i != None:\n",
    "            n = len(x.shape)\n",
    "            backdims = list(range(-1*n, 0))\n",
    "            integral += (jac * x_i).sum(dim = backdims)#Evaluate Jacobian in x_i direction\n",
    "        else:\n",
    "            integral += jac\n",
    "\n",
    "\n",
    "    return integral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4995, 0.0000],\n",
      "        [1.1770, 0.0000]])\n",
      "tensor([[1.4995, 0.0000],\n",
      "        [1.1770, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = t.Tensor([[1,2],[3,4]])\n",
    "x_i = None\n",
    "r = t.randn_like(x)\n",
    "\n",
    "y = lambda z: r@z\n",
    "j = integrate_gradient(x,x_i, y, steps = 20)\n",
    "steps = 20\n",
    "\n",
    "\n",
    "n = len(x.shape)\n",
    "backdims = list(range(-1*n, 0))\n",
    "x_i = t.zeros_like(x)\n",
    "x_i[0,0] = 1\n",
    "\n",
    "print((j * x_i).sum(dim = backdims)) #Evaluate Jacobian in x_i direction\n",
    "print(integrate_gradient(x,x_i, y, steps = 20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-10, -9, -8, -7, -6, -5, -4, -3, -2, -1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(-10,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum(a,b,c):\n",
    "    return a + b + c\n",
    "sum(*[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_name = 'standardx8.shakespeare_64x4'\n",
    "name = 'standard.shakespeare_64x4'\n",
    "data_dir = 'data/shakespeare'\n",
    "batch_size = 1\n",
    "config = sae_options[c_name]\n",
    "config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
