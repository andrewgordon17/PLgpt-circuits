import torch as t
import torch.nn as nn
from torch import Tensor
from torch.autograd.functional import jacobian
import os



from models.gpt import GPT
from models.sparsified import SparsifiedGPT, SparsifiedGPTOutput

from config.gpt.training import options
from config.sae.models import sae_options



from data.tokenizers import ASCIITokenizer, TikTokenTokenizer

from models.sae import SparseAutoencoder
from typing import Callable
from datasets import Dataset
from data.dataloaders import TrainingDataLoader
TensorFunction = Callable[[Tensor], Tensor]

def ig_attributions(model: SparsifiedGPT, layer0: int, layer1: int, ds: TrainingDataLoader, nbatches: int=32):
    assert layer0 < layer1
    assert layer0 >= 0
    assert layer1 <= model.gpt.config.n_layer
    sae0 = model.saes[f'{layer0}']
    sae1 = model.saes[f'{layer1}']
    #pdb.set_trace()

    #define function that goes from feature magnitudes in layer0 to feature magnitudes in layer1
    class Sae0Decode(nn.Module):
        def forward(self, x):
            return sae0.decode(x)
        
    class Sae1Encode(nn.Module):
        def forward(self, x):
            return sae1.encode(x)
    
    forward_list = [Sae0Decode()] + [model.gpt.transformer.h[i] for i in range(layer0, layer1)] + [Sae1Encode()]
    forward = t.nn.Sequential(*forward_list)

    
    attributions = 0
    for _ in range(nbatches):

        input, _ = ds.next_batch(model.gpt.config.device) #get batch of inputs
        
        input = input.to(t.int32).clone() #hacky to make thigs work on my M3, remove later
        output = model.forward(input, targets=None, is_eval=True)
        feature_magnitudes0 = output.feature_magnitudes[layer0] #feature magnitudes at source layer (batchsize, seqlen, sae_encoding_dim)
        
        
        jacobian = integrate_gradient(feature_magnitudes0, None, forward) #(batch seq dim batch seq dim)
        attributions = attributions + (jacobian **2).sum(dim=[0,1,3,4]) #Does this make sense? It feels incorrect to sum over position
            #maybe a better output would have shape (seqlen, dim, seqlen, dim)?
    
    attributions = t.sqrt(attributions)
    return attributions

def integrate_gradient(x: Tensor, x_i: Tensor | None, y: TensorFunction, base:int= 0, steps:int=10):
    """
    Approximates int_C d/dx_i y(z) dz where C is a linear path from base to x
    :param x: End of path. It can be either a vector of feature magnitudes generated by the data, 
        or a one hot encoding of a feature magnitude. It should have a nonzero entry for x_i (???)
        Shape (batchsize, seq_len, encoding)
    :param x_i: Direction of partial derivative. It is often a one hot encoding of a given feature magnitude. 
        If none, this computes the entire Jacobian
        Shape (batchzise, seq_len, encoding) ??Support just encoding??
    :param y: Tensor valued function on the vector space containing x
    :param base: Start of path. Default to 0
    :param steps: Number of steps to use to approximate the integral
    :return: Tensor of shape ((output of y), x.shape) if x_i = None else shape (output of y)

    
    """
    
    if base == 0:
        base = t.zeros_like(x)
    path = t.linspace(0, 1, steps)
    steplength = t.linalg.norm(x - base, dim = -1, keepdim = True)/steps

    integral = 0
    for alpha in path:
        point = (alpha*x + (1-alpha)*base).detach() #Find point on path
        jac = jacobian(y, point)*steplength #compute jacobian of y wrt x, scale by length of a step
        
        if x_i != None:
            n = len(x.shape)
            backdims = list(range(-1*n, 0))
            integral += (jac * x_i).sum(dim = backdims)#Evaluate Jacobian in x_i direction
        else:
            integral += jac


    return integral


def send_to_cpu(model):
    device = t.device("cpu")
    for _, param in model.named_parameters():
        param.to(device)
    for saekey in model.saes:
        model.saes[saekey].to(device)
    model.gpt.config.device = device

if __name__ == "__main__":
   #This code loads a model and data, and runs attrbution on the layers 0 to 1
    c_name = 'standardx8.shakespeare_64x4'
    name = 'standard.shakespeare_64x4'
    data_dir = 'data/shakespeare'
    batch_size = 32
    config = sae_options[c_name]
    all_cpu = False
    device = t.device("cpu") if all_cpu else config.device


    model = SparsifiedGPT(config)
    model_path = os.path.join("checkpoints", name)
    model = model.load(model_path, device=config.device)


    ddp = int(os.environ.get("RANK", -1)) != -1  # is this a ddp run?
    if ddp:
        # use of DDP atm demands CUDA, we set the device appropriately according to rank
        ddp_rank = int(os.environ["RANK"])
        ddp_local_rank = int(os.environ["LOCAL_RANK"])
        ddp_world_size = int(os.environ["WORLD_SIZE"])
        device = t.device(f"cuda:{ddp_local_rank}")

        assert t.cuda.is_available()
        t.cuda.set_device(device)
    else:
        # vanilla, non-DDP run
        ddp_rank = 0
        ddp_local_rank = 0
        ddp_world_size = 1
        device = config.device

    dataloader = TrainingDataLoader(
        dir_path=data_dir,
        B= batch_size,
        T=model.config.block_size,
        process_rank=ddp_rank,
        num_processes=ddp_world_size,
        split="val",
    )
    x,y = dataloader.next_batch(device)

    
    layer0 = 0
    layer1 = 1

    if all_cpu:
        send_to_cpu(model)

    attributions = ig_attributions(model, 0, 1, dataloader)
    print("hi")
    x = attributions.shape

    print(attributions.shape)
    



   






